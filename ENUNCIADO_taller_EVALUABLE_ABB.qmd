---
title: "ENUNCIADO taller en grupo Mat3 GIN 2025-2026"
author: "Taller"
lang: es
format:
  html:
    theme: superhero
    toc: true
    toc_depth: 4
    html-math-method: katex
    code-tools: true
    code-fold: true
    collapse: true
    keep-md: true
    code-overflow: wrap
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE,cache=FALSE)
# packages
library(tidyverse)
library(kableExtra)
```

# Instrucciones para el taller

Se entrega en grupos que deben de estar constituidos en la actividad de grupos. Los grupos son de 2 o 3 ESTUDIANTES, loa caso especiales consultadlos con el profesor para que los autorice.

**Enlaces y Bibliografía**

-   [R for data science, Hadley Wickham, Garret Grolemund.](https://r4ds.had.co.nz/)
-   [Fundamentos de ciencia de datos con R.](https://cdr-book.github.io/)
-   [Tablas avanzadas: kable, KableExtra.](https://haozhu233.github.io/kableExtra/awesome_table_in_html.html)
-   [Geocomputation with R, Robin Lovelace, Jakub Nowosad, Jannes Muenchow](https://r.geocompx.org/)
-   Apuntes de R-basico y tidyverse moodel MAT3.

## Objetivo MALLORCA

Leeremos los siguientes datos de la zona de etiqueta `mallorca` con el código siguiente:

```{r}
load("clean_data/mallorca/listing_common0.RData")
ls()
listings0 = listings_common0 %>%
  select(id, scrape_id, listing_url,
         neighbourhood_cleansed, price,
         number_of_reviews,
         review_scores_rating,
         review_scores_rating,
         review_scores_cleanliness,
         review_scores_location,
         review_scores_value,
         number_of_reviews,
         accommodates,
         bathrooms_text,
         bedrooms,
         beds,
         minimum_nights,
         description,
         latitude,
         longitude,
         property_type,
         room_type)
```


**listings**

Generamos la  tibble `listings0` con datos DE  8 periodos  DE  apartamentos de inside Airbnb de Mallorca Y  seleccionando cuantas variables nos parecen más interesantes.

Separararemos la fecha del scrapping que es en la que se observaron  los datos de cada apartamento  nos quedaremos con los apartamentos que aparecen en las 8 periodos "scrapeados".

```{r}
listings0= listings0 %>% 
  mutate(date=as.Date(substr(
    as.character(scrape_id),1,8),
    format="%Y%m%d"),
    .after=id)
```

Ahora  analizamos  las fechas de los scrapings y el número de veces que aparecen 
 cada  apartamentos.



```{r}
table(listings0$date)
```
Hay 8 periodos de scrapping y vamos a quedarnos con los apartamentos que aparecen en todos los periodos


Vemos que cada apartamento aparece 8 veces una por periodo.


```{r}
table(table(listings0$id))
```


Notemos que cada apartamento:

-   queda identificado por id y por date que nos da el periodo en la que apareció el dato.
-   así que cada apartamento aparece 8 veces ya que hemos elegido solo los apartamentos que aparecen en las 8 muestras.
-   Las muestras son `r unique(listings0$date)`,

```{r}
unique(listings0$date)
```

**reviews**

Estos datos necesitan leerse de forma adecuada, las columnas 1, 2 y 4 deben ser de tipo `character` las otras son correctas

```{r}
reviews=read_csv("data/mallorca/2025-09-21/reviews.csv.gz")
str(reviews)
head(reviews)
```

**neighbourhoods.csv**

Son dos columnas y la primera es una agrupación de municipios (están NA) y la segunda es el nombre del municipio

```{r}
municipios=read_csv("data/mallorca/2025-09-21/neighbourhoods.csv")
str(municipios)
head(municipios)
```

**neighbourhoods.geojson**

Es el mapa de Mallorca, o podemos leer así:

```{r}
library(sf)
library(tmap)

# Leer el archivo GeoJSON
geojson_sf <- sf::st_read("data/mallorca/2025-09-21/neighbourhoods.geojson")

# Crear un mapa

# interactivo
tmap_mode("plot") # Cambiar a modo  view/plot   que es interactivo/estático
tm_shape(geojson_sf) +
  tm_polygons(col = "cyan", alpha = 0.6) +
  tm_layout(title = "Mapa - GeoJSON Mallorca con municipios")
```

Tenéis que consultar en la documentación de inside Airbnb para saber que significa cada variable. Os puede ser útil leer los ficheros [DATA_ABB_modelo_de_datos.html](DATA_ABB_modelo_de_datos.html) y [DATA_ABB_modelo_de_datos.pdf](DATA_ABB_modelo_de_datos.html) en los que se explica el modelo de datos de inside Airbnb y como se cargan en el espacio de trabajo.

Responder las siguientes preguntas con formato Rmarkdown (.Rmd) o quarto (.qmd) y entregad la fuente un fichero en formato html como salida del informe. Se puntúa la claridad de la respuesta, la calidad de la redacción y la corrección de la respuesta.

## Pregunta 1 (**1punto**)

Del fichero con los datos de listings `listings0` calcula los estadísticos descriptivos de las variable `price` y de la variable `number_of_reviews` agrupados por municipio y por periodo.

Presenta los resultados con una tabla de kableExtra.

```{r preg1, message=TRUE, warning=TRUE}
# ---------------------------------------------------------------
# PREGUNTA 1
# Estadísticos descriptivos de:
#   - price
#   - number_of_reviews
# agrupados por:
#   - municipio (neighbourhood_cleansed)
#   - periodo (date)
# Tabla de salida con kableExtra
# ---------------------------------------------------------------

# IMPORTANTE:
# Aquí asumimos que:
#  - listings0 ya está creado según el enunciado
#  - listings0 tiene la columna 'date'
#  - 'price' es numérica (si no lo es, habría que convertirla en otro chunk)
library(dplyr)
library(kableExtra)

# 1. Calcular los estadísticos descriptivos por municipio y periodo.
#    group_by(): forma grupos por municipio y fecha de scraping
#    summarise(): calcula los resúmenes para cada grupo
resumen_p1 <- listings0 %>%
  group_by(neighbourhood_cleansed, date) %>%
  summarise(
    # Tamaño de muestra en cada grupo
    n = n(),
    
    # ---- Estadísticos para 'price' ----
    mean_price = mean(price, na.rm = TRUE),              # media
    sd_price   = sd(price, na.rm = TRUE),                # desviación estándar
    min_price  = min(price, na.rm = TRUE),               # mínimo
    q1_price   = quantile(price, 0.25, na.rm = TRUE),    # cuartil 1
    med_price  = median(price, na.rm = TRUE),            # mediana
    q3_price   = quantile(price, 0.75, na.rm = TRUE),    # cuartil 3
    max_price  = max(price, na.rm = TRUE),               # máximo
    
    # ---- Estadísticos para 'number_of_reviews' ----
    mean_reviews = mean(number_of_reviews, na.rm = TRUE),   # media
    sd_reviews   = sd(number_of_reviews, na.rm = TRUE),     # desviación estándar
    med_reviews  = median(number_of_reviews, na.rm = TRUE), # mediana
    
    # Indicar a dplyr que "desagrupe" al terminar
    .groups = "drop"
  )

# (Opcional) ver una muestra de la tabla en la consola
# head(resumen_p1)

# 2. Ordenar y mostrar la tabla con kableExtra


# Opcional: que los NA aparezcan como vacío en la tabla
options(knitr.kable.NA = "")

tabla_p1 <- resumen_p1 %>%
  arrange(neighbourhood_cleansed, date) %>%              # ordenar municipio/fecha
  mutate(across(where(is.numeric), ~ round(., 2)))       # redondear numéricos

tabla_p1 %>%
  kbl(
    caption = "Estadísticos descriptivos de price y number_of_reviews por municipio y periodo",
    col.names = c(
      "Municipio", "Periodo", "n",
      "Media", "SD", "Mín.", "Q1", "Mediana", "Q3", "Máx.",
      "Media", "SD", "Mediana"
    ),
    align    = c("l", "c", "r", rep("r", 10)),           # alineación columnas
    booktabs = TRUE
  ) %>%
  # Cabeceras agrupadas: primeras 3 columnas sin grupo,
  # luego 7 para price y 3 para number_of_reviews
  add_header_above(c(" " = 3, "Price" = 7, "Number of reviews" = 3)) %>%
  kable_styling(
    full_width        = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive")
  ) %>%
  column_spec(1, bold = TRUE) %>%                        # municipio en negrita
  collapse_rows(columns = 1, valign = "top")             # unir filas por municipio

```


## Pregunta 2 (**1punto**)

Consideremos las variables `price` y `number_of_reviews` de Pollença y Palma del periodo "2024-09-13", del fichero `listing_common0_select.RData`. 
Estudiad si estos datos se aproximan a una distribución normal gráficamente. Para ello, dibujad el histograma, la función "kernel-density" que aproxima la densidad y la densidad de la normal de media y varianza las de las muestras de las variables `price` (para precios mayores de 50 y menores de 400) y `number_of_reviews` para Palma y Pollença

```{r pregunta2, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)

# -------------------------------------------------------------------
# Supongo que ya tienes 'listings0' creado con la columna 'date'.
# Si lo cargas de otro fichero (p.ej. listing_common0_select.RData),
# simplemente asegúrate de que se llama 'listings0' y tiene:
#   - date
#   - neighbourhood_cleansed
#   - price
#   - number_of_reviews
# -------------------------------------------------------------------

# 1) Filtrar solo la fecha y los municipios de interés
datos_2 <- listings0 %>%
  filter(
    date == as.Date("2024-09-13"),
    neighbourhood_cleansed %in% c("Pollença", "Palma de Mallorca")
  )

# 2) Para PRICE solo queremos precios entre 50 y 400 €
datos_price <- datos_2 %>%
  filter(price > 50, price < 400)

# ----------------- FUNCIONES AUXILIARES DE GRÁFICO -----------------

# Histograma + densidad kernel + curva normal (misma media y sd) para PRICE
grafico_price <- function(df, titulo) {
  m <- mean(df$price, na.rm = TRUE)
  s <- sd(df$price, na.rm = TRUE)
  
  # Secuencia de x y densidad normal teórica
  x_seq <- seq(
    from = min(df$price, na.rm = TRUE),
    to   = max(df$price, na.rm = TRUE),
    length.out = 400
  )
  normal_df <- data.frame(
    x    = x_seq,
    dens = dnorm(x_seq, mean = m, sd = s)
  )
  
  ggplot(df, aes(x = price)) +
    geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.4) +
    geom_density(linewidth = 1) +
    geom_line(data = normal_df, aes(x = x, y = dens),
              linewidth = 1, linetype = "dashed") +
    labs(
      title = titulo,
      x = "Precio por noche (€)",
      y = "Densidad"
    ) +
    theme_minimal()
}

# Histograma + densidad kernel + curva normal para NUMBER_OF_REVIEWS
grafico_reviews <- function(df, titulo) {
  m <- mean(df$number_of_reviews, na.rm = TRUE)
  s <- sd(df$number_of_reviews,   na.rm = TRUE)
  
  x_seq <- seq(
    from = min(df$number_of_reviews, na.rm = TRUE),
    to   = max(df$number_of_reviews, na.rm = TRUE),
    length.out = 400
  )
  normal_df <- data.frame(
    x    = x_seq,
    dens = dnorm(x_seq, mean = m, sd = s)
  )
  
  ggplot(df, aes(x = number_of_reviews)) +
    geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.4) +
    geom_density(linewidth = 1) +
    geom_line(data = normal_df, aes(x = x, y = dens),
              linewidth = 1, linetype = "dashed") +
    labs(
      title = titulo,
      x = "Número de reviews",
      y = "Densidad"
    ) +
    theme_minimal()
}

# ----------------- SEPARAR DATOS POR MUNICIPIO -----------------

# Palma de Mallorca
palma_price <- datos_price %>%
  filter(neighbourhood_cleansed == "Palma de Mallorca")
palma_rev <- datos_2 %>%
  filter(neighbourhood_cleansed == "Palma de Mallorca")

# Pollença
pollenca_price <- datos_price %>%
  filter(neighbourhood_cleansed == "Pollença")
pollenca_rev <- datos_2 %>%
  filter(neighbourhood_cleansed == "Pollença")

# ----------------- GRÁFICOS A MOSTRAR EN EL INFORME -----------------

# 1) PRICE (50–400 €) Palma
grafico_price(
  palma_price,
  "Palma de Mallorca – price (50–400 €), 2024-09-13"
)

# 2) PRICE (50–400 €) Pollença
grafico_price(
  pollenca_price,
  "Pollença – price (50–400 €), 2024-09-13"
)

# 3) NUMBER_OF_REVIEWS Palma
grafico_reviews(
  palma_rev,
  "Palma de Mallorca – number_of_reviews, 2024-09-13"
)

# 4) NUMBER_OF_REVIEWS Pollença
grafico_reviews(
  pollenca_rev,
  "Pollença – number_of_reviews, 2024-09-13"
)
```

## Pregunta 3 (**1punto**)

Con los datos de `listings0` de todos los periodos, contrastar si la media del precio en Alcudia es igual a la de Palma **contra** que es mayor que en Palma para los precios mayores que 50 euros y menores de 400. Construid la hipótesis nula y alternativa, calculad el $p$-valor y el intervalo de confianza asociado al contraste. Justifica técnicamente la conclusión del contraste.


## Pregunta 4 (**1punto**)

Con los  datos de `listings0`, contrastar si las medias de los precios en Alcudia entre los periodos  2025-06-15 y   2025-09-21 son iguales contra que son menores en 2023. Construid la hipótesis nula y alternativa, calculad el $p$-valor y el intervalo de confianza asociado al contraste.

Haced un diagrama de caja comparativo de los precios  en Alcudia  por periodo y coméntalo.

## Pregunta 5 (**1 punto**)

Comparar con un bopxlot de las valoraciones medias `review_scores_rating` para Alcudia, Palma, Calvià y
Pollença. Hacer el gráfico con ggplot2 y todo lujo de destalles.


## Pregunta 6 (**1 punto**)

Calcular la proporción de apartamentos de la muestra "2025-09-21" con media de valoración `review_scores_rating` mayor que 4 en Alcudia y en Calvià son iguales contra que son distintas. Construid un intervalo de confianza para la diferencia de proporciones.


## Pregunta 7 (**1punto**)

Calcular la proporción de apartamentos de los periodos 2025-06-15 y  2025-09-21  con media de valoración `review_scores_rating` mayor que 4 en Palma  y en Pollença son iguales contra que son distintas.

## Pregunta 8 (**1punto**)

Agrupa las variables `review_scores_rating` y `review_scores_location` de `listings0` en 5 categorías cada una y construid una tabla de contingencia con las dos variables agrupadas. Agrupar de forma que no cruces de categorías vacías. Contratar si esta varibles son independientes con  un test $\chi^2$. 

Buscan información sobre el coeficiente de contingencia de Carl Pearson, cacularlo desde  la salida de chisq.test interpretarlo  en esta caso


```{r}
table(cut(listings0$review_scores_rating,5),
      cut(listings0$review_scores_location,5))
```



## Pregunta 9 (**3 puntos**)

Construye un data set con las variables 
review_scores_rating, review_scores_cleanliness, review_scores_location, review_scores_value de listings0 y  el municipio/zona `neighbourhood_cleansed`

Calcula la matriz de correlaciones entre estas variables y haz un gráfico de pares  de variables que muestre las correlaciones ([ggpairs](https://r-charts.com/correlation/ggpairs/)) con la librería GGally. Comenta los resultados.

Haz un `matrixplot` de las correlaciones con la librería `corrplot`. Comenta los resultados.

## Pregunta 10 (**2 puntos**)

La [Zipf's law es una ley empírica](https://en.wikipedia.org/wiki/Zipf%27s_law#Word_frequencies_in_natural_languages) que dice que la frecuencia de las palabras en un texto es inversamente proporcional a su rango. Decidid si la ley se ajusta a los datos de la longitud de los comentarios de los apartamentos de la muestra "2025-09-21" Mallorca, haced lo mismo para description de `listings0`. Para ello, haced un análisis de regresión lineal de la frecuencia de las longitudes de los comentarios/descripciones de los apartamentos de Mallorca y el rango de las longitudes de los comentarios. Justificad la respuesta, estadísticamente y gráficamente.

Como ayuda estudiar el siguiente código, utilizadlo y comentadlo.

```{r}
library(stringr)
# para las reseñas
head(reviews)
length_rewiews=stringr::str_count(reviews$comments,"\\w+")
barplot(table(length_rewiews))

#para las descripciones
length_description=stringr::str_count(listings0$description,"\\w+")
barplot(table(length_description))
```

Y ahora se calculan los rango os lo dejo para reviews par desctption lo haceís vosotros

```{r}

aux=table(length_rewiews)
head(aux)
head(names(aux))
tbl=tibble( L=as.numeric(names(aux)),Freq=as.numeric(aux),
            Rank=rank(L),Log_Freq=log(Freq),Log_Rank=log(Rank))
str(tbl)
```

```{r}
tbl2=tbl %>% filter(Rank>10) %>% filter(Rank<1000)
sol1=lm(tbl2$Freq~tbl2$Rank)
summary(sol1)

sol2=lm(tbl2$Freq~tbl2$Log_Rank)
summary(sol2)

sol3=lm(tbl2$Log_Freq~tbl2$Log_Rank)
summary(sol3)
```


